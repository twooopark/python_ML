# python_ML
Studying Machine Learning using Python

## 1. 파이썬

  - 파이썬 기본 문법
  - 연산자, 문자열, 리스트, 튜플, 딕셔너리, 함수, 클래스 등
  - python_ML/python_ml_basic/ex_0201 ~ ex_0205


## 2. 파이썬 패키지

  - Numpy
    - 배열의 연산, 행렬, 선형대수
    - python_ML/python_ml_basic/ex_0301 ~ ex_0304
  - Pandas
    - 테이블 제어, 통계, 시각화
    - python_ML/python_ml_basic/ex_0305 ~ ex_0306
  - Matplotlib 
    - python_ML/python_ml_basic/ex_0307

## 3. 지도학습 1

  - 오류 
    > 지도학습은 일반화를 검증하기 위해, 학습데이터로 만든 모형을 시험데이터로 테스트 한다.
    - 편향오류(bias error) or 과소적합 오류(underfitting error) : 한 쪽에 치우쳐(편향되어) 있는 오류, 학습목표에 미달하는 경우
    - 분산오류(variance error) or 과적합 오류(overfitting error) : 모형이 과하게 복잡하여 발생, 많은 데이터로 오류 줄일 수 있다.
    - 오류 : 편향오류 + 분산오류 + 상수
    - 오류의 최소화 : 적당한 편향오류와 적당한 분산오류의 최적점을 찾아야 한다.
    - 이러한 최적점을 찾기 위한 알고리즘으로 KNN 알고리즘과 편향-분산 트레이드 오프가 있다. 지도학습2에서 정리한다.
  
  - 선형회귀
  
    - 결정계수
    - MSE
    - 잔차
    - 레버리지
    - 실습 0401
    
  - 로지스틱회귀
  
    - 혼돈행렬
    - 민감도
    - 특이도
    - 정확도
    - 실습 0402
    
  - 나이브 베이즈
  
    - 베이즈 통계법
    - 실습 0403


## 4. 비지도학습 
  
  - k-means 클러스터 알고리즘
    - 원리 : 군집의 중심점을 찾는다. 중심점으로 반복적으로 이러한 동작을 수행하며 수렴한다.
      - 지정된 클러스터의 개수 N개 만큼 최초에 중심점의 위치 N개를 랜덤으로 선정한다.
      - 각 클러스터 내부에서 클러스터의 중심점(Centroid, 뮤)과 데이터들(Xi)의 거리가 최소화 되도록 반복한다.
      - 더 이상 위 과정에 결과가 변하지 않으면,(거리가 최소가 되었다면) 반복을 마친다.
    - 유클리드 거리(Euclidean distance)
      - 두 점 사이의 거리 계산
    - 장점 : 1. 직관적인 이해 2. 모수에 대한 추정이 필요 없다
    - 단점 : 1. 노이즈 영향 크다. 2. 외상치에 민감
    - 적절한 클러스터의 개수는 어떻게 구할까?
      - 각 클러스터의 중심점과 각 클러스터의 데이터들과의 거리의 합 : tss 
      - tss를 클러스터들에 있어서 각 클러스터 중심점과 데이터의 거리의 합이라고 한다면, 클러스터의 개수가 늘어날 수록, tss는 작아질 것이다. 하지만, 점점 작게 줄어들 것이다. 그리고, 클러스터의 개수가 많으면, 각 클러스터의 특성을 나타내는데 낭비가 생겨나게 된다. 그러므로 tss를 이용해, 변곡점(elbow)을 찾아 최적의 클러스터 개수를 구한다.
      
      ```python
      # 각 클러스터의 중심점과 각 클러스터의 데이터들과의 거리의 합 : tss(=total_ss_within) 
      def total_ss_within(X, centers, clusters):
          N_clusters = centers.shape[0]
          N_columns = centers.shape[1]
          N_rows = X.shape[0]
          ref_centers = np.zeros((N_rows, N_columns))
          for n in range(N_clusters):
              indices = (clusters == n)
              for j in range(N_columns):
                  ref_centers[indices,j] = centers[n,j]
          return np.sum((X-ref_centers)**2.0)
          
      n_cluster = np.array(range(2,20))
      total_ssw = np.array([])
      for n in n_cluster:
        kmeans = KMeans(n_clusters=n)
        clusters = kmeans.fit(X).labels_
        centers = kmeans.cluster_centers_ # 각 클러스터의 중심점
        total_ssw = np.append(total_ssw, total_ss_within(X,centers,clusters)) # 각 n개의 클러스터의 중심점~요소들 거리의 합
      ```
      
    - 활용 방안 : 1. 고객 분류  2. 주식 포트폴리오 분류 전략  3. 리스크 관리
    - 실습 0501
  
  - 계층적 군집화
    - 가까운 아이템끼리 순서대로 뭉쳐가는 형식 (DP의 Bottom up 방식처럼...)
  
  - DBScan
    - 밀도에 따라 군집을 만들어 간다. eps, minPts 파라미터를 사용한다.
    - 시작 점에서 eps(반지름)까지의 거리안에 minPts 개 이상의 데이터가 있는지 확인한다.
    - minPts개 이상의 데이터가 있다면, 군집이라고 판단하고, 그 중 한 점으로 옮긴다.
    - 이를 반복적으로 실행하면, 끊기지 않고 연결되는 좌표들이 군집을 형성한다.
  
  - 큰 원형 링안에 작은 원이 있는 형태의 데이터를 군집화 하기에 적당한 방법은?
    - DBscan으로 원형 링과 작은 원을 분류할 수 있다.
    - 실습 0502
  
  - 주성분 분석(PCA)
    - 목적 : 서로 상관관계가 있는 변수를 주성분(PC)으로 변환하는 것.
    - 변수의 값들의 변동성(분산)이 큰 순서대로 정렬을 할 수 있다.
      - 분산이 작은 순서대로 차원을 축소 해 나갈수 있다.
    - ex) PC1 = 0.5x몸무게 - 0.1x신장 + 0.4x연봉 주성분이 어떤 의미를 갖는지는 알기 어렵다(해석이 어렵다).하지만, 주성분끼리 독립성을 갖기 때문에, 분석의 성능을 높일 수 있다.
    - 데이터 전처리, 모델링, 차원축소 등에 사용된다.
      - 원리
        - PC1이 가장 큰 변동의 방향을 향하고 있고, PC2는 PC1에 직교한다.
        - 주성분(PC1과 PC2)를 새로운 좌표축으로 사용한다. 
        - PC2축(Y축)의 변수를 제거하여, 차원을 축소 한다.
        - 의문점 : 분산이 작은 순서대로 차원을 축소할까? 분산이 큰 주성분의 직교 변수를 제거할까?
    - 장점 : 가장 뚜렷한 특징만 뽑아낸다. 차원 축소를 통한 메모리, 연산 감소
    - 단점 : 주성분이 원 변수들을 섞어 만든 것이라 해석이 어렵다. 차원 축소를 통해 디테일이 손상된다.
    
  - 비음수 행렬분해(NMF)
    > 성분 벡터의 원소가 양수여야 하고ㅡ 원 데이터(주성분)를 여러 성분의 합으로 표현한다.
    
  - t-SNE
    > 가까운 좌표끼리 뭉치고, 먼 좌표끼리는 더 떨어지게 만든다. 이는 군집화를 통해 시각화를 향상시킬 수 있다.
  
  - 연관성 분석
    > 장바구니 분석이라고도 불린다.(기저귀 살때, 맥주도 같이 사는 ... 서로 독립적인 사건의 연관성 분석)
    > 여러번 발생한 이벤트나 거래에서 일정한 규칙을 찾아내는 분석이다.
    - 지지도(Support) : P(AnB), 해당 규칙이 얼마나 의미있는 규칙인지 나타냄. (전체 사건중에서 A와 B가 둘다 샀을 확률)
    - 신뢰도(Confidence) : P(B|A), A를 포함한 이벤트 중, A와 B가 동시에 발생하는 비중. (지지도 / P(A)) : 지지도가 P(A)보다 얼마나 높은지.
    - 향상도(Lift) : P(AnB)/P(A)P(B) => P(B|A)/P(B),  A와 B의 `상호관계`의 강도를 나타냄. 클수록 강한 연관성 규칙을 의미함. (신뢰도 / P(B)) : 신뢰도가 P(B)보다 얼마나 높은지.
      - ex) 맥주만 살 확률이 적을수록, 기저귀를 샀을때, 맥주를 살 확률이 높을수록, 향상도가 높다.
    - R에서 더 상세히 학습해보자.
  

## 5. 지도학습2
  - KNN(K Nearest Neighbor)알고리즘
    > 어떤 점 주변의 제일 가까운 K개의 데이터 포인트를 찾아서(이웃을 찾는다), "다수결"로 값을 정하는 방식. ( N개의 독립변수 -> N차원에서의 점 )
    - K-means와 같이 점과 점사이의 거리를 구하기 위해 유클리드 거리 측정법을 사용한다.
    - `생각) 그렇다면, 만약 학습된 모델에 천만개의 라벨이 있을 때, 결과를 예측하고자 하는 내 좌표(독립변수)와, 모델의 천만개의 거리를 다 구해서,    거리가 짧은 것들로 정렬한 다음, 내 결과를 예측해야 할까? >> 너무 많은 시간이 소요될 것 같다. 어떻게 하면 내 주변(가까운)의 좌표를 찾을 수 있을까?
    `
    - K를 작게 하면, 너무 디테일 하게 값을 정하게 되어, 과적합 오류(분산 오류)가 생긴다. 노이즈에 민감해진다.
    - 반대로, K를 크게 하면, 과소적합 오류(편향 오류)가 생긴다.
    - 장점 : 1. 간단하다.  2. 비모수적이다.(=모수에 대한 추정이 필요없다.)
  
  - 트리 알고리즘
    - 트리 알고리즘은 크게 분류형, 회귀형으로 구분한다.
      - 분류형 : 의사결정트리 (DecisionTree), 뿌리를 따라 내려가다보면 답이 구해지는 구조. 트리의 각 node의 조건은 데이터를 통한 학습으로 만들어진다.
      - 회귀형 : 분류형과 유사하게 트리의 각 node에서 질문에 대한 답에 따라 갈라 지는데, 여기서 설명변수를 `범위`로 쪼갠다. 
      ```
      Ex) 당신의 연봉은 얼마가 적절할까요?
      종속변수 : 연봉 
      독립변수 : 인적성 점수, 면접 점수, 어학 능력, 경력
      
      지도학습이므로 트레이닝을 하고, 예측을 할 것임.
      1. 트레이닝 : X값 들과 그에 대응되는 Y값 데이터들을 가지고, 범위를 나눈다.
      -> 여기서 범위를 어떻게 나누냐? 예를들어, 경력 4년부터 연봉 4천을 할건지 3년부터 4천을 할건지 ?? >> 공부해야함
      2. 트레이닝을 마친 모델을 가지고, 당신의 독립변수 값들을 입력해서, 연봉을 예측할 수 있다.~ 
      ```
      - 과적합 문제가 잘 발생하기 때문에, 교차검증이 필요할 것이다.
  
  - 랜덤포레스트와 에이다부스트
    - 랜덤포레스트(Random Forest) 
    > 비교적 "약한" 학습기인 트리를 기반으로 앙상블을 만든다. 지니불순도가 낮은 의사결정을 할 수 있게 한다.
    분류 문제에 적용하는데 유용하다. 평균값 예측에도 적용 가능하지만, 선형회귀가 더 좋다.
    
      - 장점 : 1. 높은 정확도  2. 오버피팅X  3. `PCA필요X(이게 중요한 의미를 가짐: 애초에 랜덤포레스트는 분별력이 높은 변수들만 가지고 선거인단을 만들기 때문에, 분별력이 낮은 변수들은 (PCA가 필요한 변수들은) 알고리즘을 거치면서 이미 결과 모델(모형)에 소거되어있다.)`
      - 단점 : 1. 계산시간이 김.
    
    - 에이다부스트(AdaBoost) 
    > 랜덤포레스트와 유사하지만, 두 종류의 가중치(분류기에 대한 가중치, 관측값에 대한 가중치)를 가진다.
    알고리즘 원리는 학습하지 않았음. 마치 진화론처럼 성능이 낮은 분류기들은 낮은 가중치를 받아 도태된다. 
    
    - 선거인단(모델) 목소리(예측)
      - 랜덤포레스트는 무작위 라벨링에서 지니불순도가 낮은 선거인단으로 여러번 뽑히면서, 지니불순도가 낮은 선거인단이 여러번 목소리를 낸다.
      - 에이다부스트는 분별력이 높은 선거인단의 목소리를 키워준다.
    - 실습 0602
    
  - SVM(Support Vector Machine) 
    > 해석해보면, 지지목 버팀목 기계? 무슨말이지... 분류 정확성과 분류 마진(여유, 가장자리)를 최대로 하는 알고리즘이다.
    두 집단을 분류하는 경계선을 구하는 데, 경계선과 집단의 거리(마진)를 이용한다.
    
  - 교차검증과 데이터전처리
    
    - 교차검증(Cross Validation)
      > 트레이닝 데이터만 가지고, 트레이닝 데이터를 K개로 나누어 K-1개의 트레이닝 세트와 검증용 세트로 사용하여 검증한다.
      - K-ford 방법 : 동일한 사이즈 K개로 나누어 검증한다.
      - Leave One Out 방법 : 단 하나의 관측값만 교차검증 용으로 남겨둔다. > 느림, 그냥 K-ford를 씁시다.
      
    - 데이터전처리
    
      - 표준화(Standardization) : 각 변수들의 스케일을 비슷하게 하기 위함. 각 데이터 값에서 평균을 빼고 표준편차로 나누어 표준화한다.
      > 단순하게 생각해보자, X값이 억단위로 움직이고, Y값이 1단위로 움직일때 차트를 상상해봐라
      
      - 차원 축소(PCA 등) : 불필요한 설명변수(=독립변수=차원)을 제거한다. (원데이터를 차원축소하고(전!처리하고), 알고리즘 돌려야 함) 
      > 실습 0503을 보면, 63차원의 데이터를 5차원으로 줄여도 식별이 가능함을 볼 수 있다. (불필요 인자 제거)
      
    - 실습 sol_0604, mini_project_0601
    
    

## 6. 손글씨 인식
  - 데이터세트
    - MNIST 데이터 베이스에서 가져온 손글씨 이미지 데이터.
    - 해상도 28 × 28의 숫자 이미지.
    - 개개의 행 = 1 + 28 × 28 (첫번 째 값이 Label).
    - 데이터 갯수를 늘리기 위해, rotation 방법 사용(이미지를 돌린다.)
    - scipy.ndimage.interpolation의 rotate함수 사용
    
  - 과제 진행 순서:
    - 이미지를 보여주는 함수를 코딩한다. 힌트: matplotlib의 imshow 함수 사용.
    - 데이터를 읽어온다: “data_mnist_train_100.csv” 등.
    - 머신러닝 방법을 적용해서 training과 testing을 하고 정확도를 계산해 본다.
    - 비교적 낮은 정확도를 향상시킬 수 있는 방법을 적용해 본다.
